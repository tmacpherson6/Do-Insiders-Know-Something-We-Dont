{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgqOCkqhXNor"
   },
   "source": [
    "# Scraping Yahoo Finance for Price Data\n",
    "This notebook will take the large 'notebook1_insider_data.csv' file and uses it to query the yahoo finance API to pull out stock and SPY price data. The goal is to obtain all historical price data for each unique symbol, calculate a 28 day moving average of close prices to see a smoothed trend of price data. We will then look at the price a month prior to the insider transaction, at the insider transaction date and every month after for six months. Along with the price data, we will calculate an average rate of change for the moving average over that monthly period. (i.e. the stock price has increased 0.2% per day for the month between transction and the close price one month later). It will add all of this data dynmically to the initial dataset and then save an intermediate .csv file that can be used for replication and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpHcNbIzYlbI"
   },
   "source": [
    "# Section 1: Importing Libraries and Capturing Dependencies\n",
    "This section will hold all of the libraries that we will be using for data import, manipulation, and analysis. We will then capture the versions of all libriaries for reproducibility with our code. Google colab uses an outdated yfinance API, so we need to explicity install the upgraded library first and restart the kernel. If using VSCode or another local IDE, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16821,
     "status": "ok",
     "timestamp": 1749216012458,
     "user": {
      "displayName": "Thomas Macpherson",
      "userId": "08726222529195553341"
     },
     "user_tz": 360
    },
    "id": "NFFChccHZJ0A",
    "outputId": "2847e952-f98e-4d85-df27-b48000af2259"
   },
   "outputs": [],
   "source": [
    "# Let's explicity update yfiance just incase\n",
    "%pip install yfinance --upgrade --no-cache-dir\n",
    "\n",
    "# A kernel restart may be necessary after running this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuyRR59hY1Lu"
   },
   "outputs": [],
   "source": [
    "# Library import\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvsRBnQBZhG_"
   },
   "source": [
    "Let's print out the versions that we are using (or save a file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3502,
     "status": "ok",
     "timestamp": 1749137931471,
     "user": {
      "displayName": "Thomas Macpherson",
      "userId": "08726222529195553341"
     },
     "user_tz": 360
    },
    "id": "FdYFYnvEZmio",
    "outputId": "af8f43d4-dc03-4cb1-b656-6fadbda73aa4"
   },
   "outputs": [],
   "source": [
    "# print the library versions\n",
    "!conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9GK096bZFF-"
   },
   "source": [
    "# Section 2: Importing Insider Transaction Data and Cleaning\n",
    "In this section we will import the data from a .CSV file that combines all insider transactions from Q1 2006 to Q1 2025. We will be using this file to pull as much price data from yfinance as we can and will be the basis of our combined datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1749137931493,
     "user": {
      "displayName": "Thomas Macpherson",
      "userId": "08726222529195553341"
     },
     "user_tz": 360
    },
    "id": "PmrFDjpgZAYS",
    "outputId": "146f3d5e-51d1-42ba-cfc7-5d457dae4e66"
   },
   "outputs": [],
   "source": [
    "# Start by taking a quick look at the files in our directory so we can pull the right one\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6018,
     "status": "ok",
     "timestamp": 1749137937531,
     "user": {
      "displayName": "Thomas Macpherson",
      "userId": "08726222529195553341"
     },
     "user_tz": 360
    },
    "id": "9cdAPZe3avKs",
    "outputId": "a1113dc7-5598-49cf-dcba-f2d1366174b7"
   },
   "outputs": [],
   "source": [
    "# Let's create a function to clean up data and visualize some information\n",
    "def read_data(file_path):\n",
    "    # Read in the .csv file\n",
    "    cs_df = pd.read_csv(file_path)\n",
    "    print(f\"Let's take a look at the size of our dataframe: {cs_df.shape}\\n\")\n",
    "    # print(cs_df.head())\n",
    "\n",
    "    # Let's start by removing any data where tickers == None\n",
    "    cs_df = cs_df[cs_df[\"Ticker\"] != \"NONE\"]\n",
    "    cs_df = cs_df[cs_df[\"Ticker\"] != \"None\"]\n",
    "\n",
    "    # Let's also drop any column that is missing the ticker or the price per share or any shares purchased\n",
    "    cs_df = cs_df[~cs_df[\"Ticker\"].isna()].copy()\n",
    "    cs_df = cs_df[~cs_df[\"Price per Share\"].isna()].copy()\n",
    "    cs_df = cs_df[cs_df[\"Shares\"] > 0.0]\n",
    "\n",
    "    # Let's fill missing insider titles, names and issuers with a string for the group by statement\n",
    "    cs_df[[\"Insider Title\", \"Insider Name\", \"Issuer\"]] = cs_df[\n",
    "        [\"Insider Title\", \"Insider Name\", \"Issuer\"]\n",
    "    ].fillna(\"Missing\")\n",
    "    print(f\"After removing transction with no tickers we have {cs_df.shape}\\n\")\n",
    "\n",
    "    # Let's take a look at the number of unique tickers in this file\n",
    "    tickers = list(cs_df[\"Ticker\"].unique())\n",
    "    print(f\"We have {len(tickers)} unique tickers\\n\")\n",
    "\n",
    "    # We can actually delete this tickers list and set it up as a generator to be more \n",
    "    # memory efficient for our loop\n",
    "    del tickers\n",
    "    tickers = (sym for sym in cs_df[\"Ticker\"].unique())\n",
    "\n",
    "    # Let's take a look at the number of missing values in the file\n",
    "    missing_counts = cs_df.isna().sum()\n",
    "    print(missing_counts)\n",
    "\n",
    "    return cs_df, tickers\n",
    "\n",
    "\n",
    "cs_df, tickers = read_data(\"notebook1_insider_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRtHuRoKc4Kb"
   },
   "source": [
    "There are multiple transactions that occur by the same insider on the same day. This is to account for shares purchased at different prices. We can combine these together using groupby in order to reduce the size of the dataframe and our compute. First, we need to calculate the average price per share. We will do this by multiply the 'Shares' column by the 'Price per Share' column to see the total capital invested for the day. We will then divide that by the total capital to get 'average price per share' after our grouping. We will group by the categorical columns, the numerical columns will have different aggregation functions based on what they represent. Shares will be summed for the day, We will take the average price per share (but likely wont use this as it wont be accurate), we will take the max of 'Shares After' to see the highest amount of shares they had at the end of the transaction period and we will sum the total capital invested to find the true average cost per share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 2526,
     "status": "ok",
     "timestamp": 1749137940061,
     "user": {
      "displayName": "Thomas Macpherson",
      "userId": "08726222529195553341"
     },
     "user_tz": 360
    },
    "id": "cuJCZqouduAj",
    "outputId": "171f696d-ec8b-4701-c4f3-aa78c57c920a"
   },
   "outputs": [],
   "source": [
    "# Let's create a function to group the transactions by day\n",
    "\n",
    "\n",
    "def daily_transactions(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and calculates the total capital invested for the transaction. All data is then aggregated based on characteristics like name, title, date, etc. The average price per share is calculated for the transaction date and a grouped dataframe is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    # Let's calcualte the capital invested for each individual transaction\n",
    "    df[\"total_capital\"] = df[\"Shares\"] * df[\"Price per Share\"]\n",
    "\n",
    "    # Let's group the dataframe so that we get all transactions on the same day by the same insider as a single transaction\n",
    "    grouped_df = df.groupby(\n",
    "        [\n",
    "            \"Insider Name\",\n",
    "            \"Insider Title\",\n",
    "            \"Insider Role\",\n",
    "            \"Issuer\",\n",
    "            \"Ticker\",\n",
    "            \"CIK Code\",\n",
    "            \"Period of Report\",\n",
    "            \"Transaction Date\",\n",
    "            \"Security\",\n",
    "            \"Transaction Code\",\n",
    "            \"Ownership Type\",\n",
    "            \"ACCESSION_NUMBER\",\n",
    "        ],\n",
    "        as_index=False,\n",
    "    ).agg(\n",
    "        shares=(\"Shares\", \"sum\"),\n",
    "        price_per_share=(\"Price per Share\", \"mean\"),\n",
    "        shares_after=(\"Shares After\", \"max\"),\n",
    "        total_capital=(\"total_capital\", \"sum\"),\n",
    "    )\n",
    "    # Now, we can find the average price per share\n",
    "    grouped_df[\"average_price_per_share\"] = (\n",
    "        grouped_df[\"total_capital\"] / grouped_df[\"shares\"]\n",
    "    )\n",
    "    # print(grouped_df.shape)\n",
    "    return grouped_df\n",
    "\n",
    "\n",
    "grouped_df = daily_transactions(cs_df)\n",
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHDDmtD4e-Z2"
   },
   "source": [
    "Finally, for ease we will prepare this dataframe for our new data columns that we will be adding. It will be easier to create them now and fill them in as we iterate through the symbols to avoid throwing errors. So we will do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "executionInfo": {
     "elapsed": 125,
     "status": "ok",
     "timestamp": 1749137940187,
     "user": {
      "displayName": "Thomas Macpherson",
      "userId": "08726222529195553341"
     },
     "user_tz": 360
    },
    "id": "AdxFslCUfN8Z",
    "outputId": "dd901561-5517-4379-aee2-6cbfa8725176"
   },
   "outputs": [],
   "source": [
    "# Let's create a list of columns to add\n",
    "new_columns = [\n",
    "    \"price_-1month\",\n",
    "    \"trend_-1month\",\n",
    "    \"transactiondate_high\",\n",
    "    \"transactiondate_low\",\n",
    "    \"trend_transactiondate\",\n",
    "    \"price_1month\",\n",
    "    \"trend_1month\",\n",
    "    \"price_2month\",\n",
    "    \"trend_2month\",\n",
    "    \"price_3month\",\n",
    "    \"trend_3month\",\n",
    "    \"price_4month\",\n",
    "    \"trend_4month\",\n",
    "    \"price_5month\",\n",
    "    \"trend_5month\",\n",
    "    \"price_6month\",\n",
    "    \"trend_6month\",\n",
    "]\n",
    "\n",
    "\n",
    "# Create a function to add columns to our dataframe\n",
    "def add_columns(df, columns):\n",
    "    for col in new_columns:\n",
    "        df[col] = pd.NA\n",
    "    return df\n",
    "\n",
    "\n",
    "# Run the function to add our colums\n",
    "grouped_df = add_columns(grouped_df, new_columns)\n",
    "\n",
    "# let's take a look to make sure it workd\n",
    "print(grouped_df.shape)\n",
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cumsn1KofTjv"
   },
   "source": [
    "# Section 3: Yahoo Finance API and Data Aggregation\n",
    "The data frame is ready for us to query the yfinance API to start pulling in data. This will likely take a significant period of time due to how much data we are trying to pull, I am going to use a magic command to time how long this truly takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AS0Z_BPGfROG"
   },
   "outputs": [],
   "source": [
    "# Let's make a copy of the dataframe because we will be manipulating it\n",
    "df = grouped_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImLDhTLifkM6"
   },
   "outputs": [],
   "source": [
    "def get_price_data(df, tickers):\n",
    "    # Now we can use the ticker to request that complete historical data for this ticker\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # The first the we need to do is find all of the transactions with this ticker\n",
    "            print(f\"\\nProcsessing ticker {ticker}...\")\n",
    "            temp_df = df.loc[df[\"Ticker\"] == ticker].copy()\n",
    "            # Now that we have a temporary dataframe, we can pull the price data\n",
    "            ticker_data = yf.download(\n",
    "                tickers=ticker,\n",
    "                period=\"max\",\n",
    "                interval=\"1d\",\n",
    "                auto_adjust=True,\n",
    "                actions=False,\n",
    "                threads=False,\n",
    "            )\n",
    "            # Let's calculate the moving avearge\n",
    "            ticker_data[\"28MA\"] = ticker_data[\"Close\"].rolling(window=28).mean()\n",
    "            # Let's normalize it based on the previous value of the MA\n",
    "            ticker_data[\"MA_diff\"] = ticker_data[\"28MA\"].pct_change()\n",
    "            ticker_data = ticker_data.dropna().copy()\n",
    "            # Finally, I want to take the rolling average of that MA_diff (momentum of momentum change) over the last 28 days so we get the monthly trend\n",
    "            ticker_data[\"MA_trend\"] = ticker_data[\"MA_diff\"].rolling(window=28).mean()\n",
    "            ticker_data = ticker_data.dropna().copy()\n",
    "            ticker_data.index = pd.to_datetime(ticker_data.index)\n",
    "            # print(ticker_data.head())\n",
    "\n",
    "            # Now, we will have to iterate through the rows to fill them out individually\n",
    "            for index, row in temp_df.iterrows():\n",
    "                # This is where we will need to grab the values and the index and populate the orginal dataframe (grouped_df)\n",
    "                # Find out the date of the transaction (specify the format explicitly)\n",
    "                trans_date = pd.to_datetime(row[\"Transaction Date\"], format=\"%d-%b-%Y\")\n",
    "                # Let's start by defining all of our dates\n",
    "                date_premonth = trans_date - pd.DateOffset(months=1)\n",
    "                date_onemonth = trans_date + pd.DateOffset(months=1)\n",
    "                date_twomonth = trans_date + pd.DateOffset(months=2)\n",
    "                date_threemonth = trans_date + pd.DateOffset(months=3)\n",
    "                date_fourmonth = trans_date + pd.DateOffset(months=4)\n",
    "                date_fivemonth = trans_date + pd.DateOffset(months=5)\n",
    "                date_sixmonth = trans_date + pd.DateOffset(months=6)\n",
    "\n",
    "                # Let's Get all of the prices at the right timepoints we will use .asof because we likely only have trading days\n",
    "                price_premonth = float(ticker_data[\"Close\"][ticker].asof(date_premonth))\n",
    "                price_onemonth = float(ticker_data[\"Close\"][ticker].asof(date_onemonth))\n",
    "                price_twomonth = float(ticker_data[\"Close\"][ticker].asof(date_twomonth))\n",
    "                price_threemonth = float(\n",
    "                    ticker_data[\"Close\"][ticker].asof(date_threemonth)\n",
    "                )\n",
    "                price_fourmonth = float(\n",
    "                    ticker_data[\"Close\"][ticker].asof(date_fourmonth)\n",
    "                )\n",
    "                price_fivemonth = float(\n",
    "                    ticker_data[\"Close\"][ticker].asof(date_fivemonth)\n",
    "                )\n",
    "                price_sixmonth = float(ticker_data[\"Close\"][ticker].asof(date_sixmonth))\n",
    "\n",
    "                # print(f\"The price is {price_premonth,price_onemonth,price_twomonth,price_threemonth,price_fourmonth,price_fivemonth,price_sixmonth}\")\n",
    "\n",
    "                # Let's bring in the high and low of the transaction date so we can verify the buy price was on that day\n",
    "                high_transactiondate = float(\n",
    "                    ticker_data[\"High\"][ticker].asof(trans_date)\n",
    "                )\n",
    "                low_transactiondate = float(ticker_data[\"Low\"][ticker].asof(trans_date))\n",
    "                # print(high_transactiondate,low_transactiondate)\n",
    "\n",
    "                # Let's get the momentum of all the trends\n",
    "                trend_premonth = ticker_data[\"MA_trend\"].asof(date_premonth)\n",
    "                trend_transactiondate = float(ticker_data[\"MA_trend\"].asof(trans_date))\n",
    "                trend_onemonth = float(ticker_data[\"MA_trend\"].asof(date_onemonth))\n",
    "                trend_twomonth = float(ticker_data[\"MA_trend\"].asof(date_twomonth))\n",
    "                trend_threemonth = float(ticker_data[\"MA_trend\"].asof(date_threemonth))\n",
    "                trend_fourmonth = float(ticker_data[\"MA_trend\"].asof(date_fourmonth))\n",
    "                trend_fivemonth = float(ticker_data[\"MA_trend\"].asof(date_fivemonth))\n",
    "                trend_sixmonth = float(ticker_data[\"MA_trend\"].asof(date_sixmonth))\n",
    "\n",
    "                # Grab todays date\n",
    "                today = pd.to_datetime(date.today())\n",
    "\n",
    "                # Let's see if we can change the original dataframe with these new values (we will use .at because its faster and replaces copies)\n",
    "\n",
    "                df.at[index, \"price_-1month\"] = price_premonth\n",
    "                df.at[index, \"transactiondate_high\"] = high_transactiondate\n",
    "                df.at[index, \"transactiondate_low\"] = low_transactiondate\n",
    "\n",
    "                if date_onemonth < today:\n",
    "                    df.at[index, \"price_1month\"] = price_onemonth\n",
    "                if date_twomonth < today:\n",
    "                    df.at[index, \"price_2month\"] = price_twomonth\n",
    "                if date_threemonth < today:\n",
    "                    df.at[index, \"price_3month\"] = price_threemonth\n",
    "                if date_fourmonth < today:\n",
    "                    df.at[index, \"price_4month\"] = price_fourmonth\n",
    "                if date_fivemonth < today:\n",
    "                    df.at[index, \"price_5month\"] = price_fivemonth\n",
    "                if date_sixmonth < today:\n",
    "                    df.at[index, \"price_6month\"] = price_sixmonth\n",
    "\n",
    "                # Let's add the price trend data as well\n",
    "                df.at[index, \"trend_-1month\"] = trend_premonth\n",
    "                df.at[index, \"trend_transactiondate\"] = trend_transactiondate\n",
    "                if date_onemonth < today:\n",
    "                    df.at[index, \"trend_1month\"] = trend_onemonth\n",
    "                if date_twomonth < today:\n",
    "                    df.at[index, \"trend_2month\"] = trend_twomonth\n",
    "                if date_threemonth < today:\n",
    "                    df.at[index, \"trend_3month\"] = trend_threemonth\n",
    "                if date_fourmonth < today:\n",
    "                    df.at[index, \"trend_4month\"] = trend_fourmonth\n",
    "                if date_fivemonth < today:\n",
    "                    df.at[index, \"trend_5month\"] = trend_fivemonth\n",
    "                if date_sixmonth < today:\n",
    "                    df.at[index, \"trend_6month\"] = trend_sixmonth\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract data for {ticker} from Yahoo Finance: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The following cell takes approximatly two hours to run. Make sure that your local environment does not sleep during this time or the call will be aborted. Consider using the output CSV, 'notebook3_added_price_data.csv', that is already produced in the local drive to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "1EW1l7Z7pE1lukFK2N7c3hGRS7d7Ze3hR"
    },
    "executionInfo": {
     "elapsed": 4188952,
     "status": "ok",
     "timestamp": 1749142129431,
     "user": {
      "displayName": "Thomas Macpherson",
      "userId": "08726222529195553341"
     },
     "user_tz": 360
    },
    "id": "x79p9Laeo3eS",
    "outputId": "72aa090c-d935-493b-80a5-59fdb950e63b"
   },
   "outputs": [],
   "source": [
    "df = get_price_data(df, tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URtV8aLDfuqD"
   },
   "source": [
    "We came across a significant number of errors due to tickers being delisted, etc through the financial crisis and other times of significant market constriction. We had expected this. Upon reruns, we obtained data for the same tickers and we tested for rate limit issues on the API. We will do a rough calculation on the number of valid tickers we were able to obtain to make sure we don't have too much of a bias.\n",
    "\n",
    "We started with 7201 unique tickers stored in the variable 'tickers'. Let's see the size of our dataframe and how many tickers we were able to keep after removing data that we were unable to pull. We are determining tickers that we were unable to pull by looking for NA values in the 'price--1month' column as that is the first datapoint to be filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1749142129813,
     "user": {
      "displayName": "Thomas Macpherson",
      "userId": "08726222529195553341"
     },
     "user_tz": 360
    },
    "id": "ytvLYT_LgDoh",
    "outputId": "17f78dbd-70a4-4a97-d710-3ede26a37a0c"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1749142129831,
     "user": {
      "displayName": "Thomas Macpherson",
      "userId": "08726222529195553341"
     },
     "user_tz": 360
    },
    "id": "Qy5kvO9kgigs",
    "outputId": "f0455122-db78-44dd-f361-259f578f9956"
   },
   "outputs": [],
   "source": [
    "def removing_failed_tickers(df):\n",
    "    print(f\"We originally had {grouped_df.shape[0]} rows\")\n",
    "    filtered_df = df[~df[\"price_-1month\"].isna()]\n",
    "    print(f\"We were able to obtain data for {filtered_df.shape[0]} rows\")\n",
    "    uni_tick = filtered_df[\"Ticker\"].unique()\n",
    "    print(f\"We now have {len(uni_tick)} unique tickers\")\n",
    "    print(f\"We kept {np.round(len(uni_tick) / len(tickers) * 100, 1)}% of the tickers\")\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "filtered_df = removing_failed_tickers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1749142129849,
     "user": {
      "displayName": "Thomas Macpherson",
      "userId": "08726222529195553341"
     },
     "user_tz": 360
    },
    "id": "i9yt0F8N3oa8",
    "outputId": "caaec67f-6b1f-4885-a3e3-93ad7117e6d4"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the number of missing values in the file\n",
    "missing_counts = filtered_df.isna().sum()\n",
    "print(missing_counts)\n",
    "\n",
    "# Let's look at a random sample of 15 of these entries\n",
    "filtered_df.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFhOb6-5iIKN"
   },
   "source": [
    "Now, that we have cleaned up our dataframe and filtered it down to only the transactions we can get data for, let's pull in all of the SPY data for each transaction. Once again, we will create a copy of the dataframe and then prepare it to add the SPY data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1749142129872,
     "user": {
      "displayName": "Thomas Macpherson",
      "userId": "08726222529195553341"
     },
     "user_tz": 360
    },
    "id": "GeOLjhFFiWB_",
    "outputId": "7b63f6e8-5442-46ab-cd71-6c3c46da9711"
   },
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe\n",
    "new_df = filtered_df.copy()\n",
    "# Create a list of columns to add\n",
    "new_columns = [\n",
    "    \"spy_price_-1month\",\n",
    "    \"spy_trend_-1month\",\n",
    "    \"spy_price_transactiondate\",\n",
    "    \"spy_trend_transactiondate\",\n",
    "    \"spy_price_1month\",\n",
    "    \"spy_trend_1month\",\n",
    "    \"spy_price_2month\",\n",
    "    \"spy_trend_2month\",\n",
    "    \"spy_price_3month\",\n",
    "    \"spy_trend_3month\",\n",
    "    \"spy_price_4month\",\n",
    "    \"spy_trend_4month\",\n",
    "    \"spy_price_5month\",\n",
    "    \"spy_trend_5month\",\n",
    "    \"spy_price_6month\",\n",
    "    \"spy_trend_6month\",\n",
    "]\n",
    "\n",
    "# Run the function previously created to add our colums\n",
    "cs_df = add_columns(new_df, new_columns)\n",
    "\n",
    "# Let's take a look quickly\n",
    "print(cs_df.shape)\n",
    "cs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7h89cZY3kd2M"
   },
   "source": [
    "# Section 4: Yahoo Finance API and Data Aggregatoin\n",
    "The dataframe is prepared for us to make one query to yahoo finance and get all market data for the SPY ETF. It shouldn't take long for us to download this data and apply some easy operations to obtain the trend data. I will iterate over all rows using .iterrows() in order to populate the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1749142129890,
     "user": {
      "displayName": "Thomas Macpherson",
      "userId": "08726222529195553341"
     },
     "user_tz": 360
    },
    "id": "B1rW2-SFkiE9",
    "outputId": "33167e9f-1cc9-4d03-eb76-c65f23c10345"
   },
   "outputs": [],
   "source": [
    "# Let's make a function to get spy data\n",
    "def get_spy_data():\n",
    "    # Let's get the spy dataframe and calculate the momentum of the trends\n",
    "    ticker = \"SPY\"\n",
    "    print(f\"\\nProcessing ticker {ticker}...\")\n",
    "    # Let's be sure to stay consistent with our ticker data calls\n",
    "    spy_data = yf.download(\n",
    "        tickers=ticker,\n",
    "        period=\"max\",\n",
    "        interval=\"1d\",\n",
    "        auto_adjust=True,\n",
    "        actions=False,\n",
    "        threads=False,\n",
    "    )\n",
    "    # Let's calculate the moving averages\n",
    "    spy_data[\"28MA\"] = spy_data[\"Close\"].rolling(window=28).mean()\n",
    "    # Normalize it based on the previous days MA for comparisons\n",
    "    spy_data[\"MA_diff\"] = spy_data[\"28MA\"].pct_change()\n",
    "    # let's get rid of the first 29days because they dont have an MA_diff\n",
    "    spy_data = spy_data.dropna().copy()\n",
    "    # Finally, lets catch the monthly trend of this moving average\n",
    "    spy_data[\"MA_trend\"] = spy_data[\"MA_diff\"].rolling(window=28).mean()\n",
    "    # Let's drop the missing data again\n",
    "    spy_data = spy_data.dropna().copy()\n",
    "    # Let's explicitly make sure the date is in the proper format\n",
    "    spy_data.index = pd.to_datetime(spy_data.index)\n",
    "    return spy_data\n",
    "\n",
    "\n",
    "spy_data = get_spy_data()\n",
    "\n",
    "print(f\"\\nThe shape of our dataframe is {spy_data.shape}\\n\")\n",
    "print(\n",
    "    f\"The first date is {min(spy_data.index)} and the last day is {max(spy_data.index)}\"\n",
    ")\n",
    "spy_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1etJI4V1k7-w"
   },
   "source": [
    "The spy data clearly covers the necessary timeframe that we are looking at, dating back to 1993. So that is great. Now we can start populating our data frame.\n",
    "\n",
    "# Section 5: Mering the data\n",
    "Let's make sure that we are using copies of the data so that we don't accidently edit our data, we will do this prior to timing our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekBj1rfVlLG2"
   },
   "outputs": [],
   "source": [
    "temp_full1 = cs_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vagsyqHylNiH"
   },
   "source": [
    "We will be iterating over 100,000+ rows. This is relatively small compared to what we are capable of doing, so it shouldn't take too much time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Ebj-FG3lAIh"
   },
   "outputs": [],
   "source": [
    "# Let's create a function to merge the data\n",
    "def merge_spy_data(df, spy_data, ticker):\n",
    "    # Set up our looping function.\n",
    "    for index, row in df.iterrows():\n",
    "        # Find the original transaction data\n",
    "        trans_date = pd.to_datetime(row[\"Transaction Date\"])\n",
    "        # Let's define all of the other dates we will look for in the spy_data\n",
    "        date_premonth = trans_date - pd.DateOffset(months=1)\n",
    "        date_onemonth = trans_date + pd.DateOffset(months=1)\n",
    "        date_twomonth = trans_date + pd.DateOffset(months=2)\n",
    "        date_threemonth = trans_date + pd.DateOffset(months=3)\n",
    "        date_fourmonth = trans_date + pd.DateOffset(months=4)\n",
    "        date_fivemonth = trans_date + pd.DateOffset(months=5)\n",
    "        date_sixmonth = trans_date + pd.DateOffset(months=6)\n",
    "        # Let's grab all of the price data from the spy_data. The initial data is double indexed so use [ticker] to get access to the data\n",
    "        price_premonth = np.round(spy_data[\"Close\"][ticker].asof(date_premonth), 2)\n",
    "        price_transactiondate = np.round(spy_data[\"Close\"][ticker].asof(trans_date), 2)\n",
    "        price_onemonth = np.round(spy_data[\"Close\"][ticker].asof(date_onemonth), 2)\n",
    "        price_twomonth = np.round(spy_data[\"Close\"][ticker].asof(date_twomonth), 2)\n",
    "        price_threemonth = np.round(spy_data[\"Close\"][ticker].asof(date_threemonth), 2)\n",
    "        price_fourmonth = np.round(spy_data[\"Close\"][ticker].asof(date_fourmonth), 2)\n",
    "        price_fivemonth = np.round(spy_data[\"Close\"][ticker].asof(date_fivemonth), 2)\n",
    "        price_sixmonth = np.round(spy_data[\"Close\"][ticker].asof(date_sixmonth), 2)\n",
    "        # print(price_premonth,price_transactiondate,price_sixmonth)\n",
    "        # Let's get the momentum of all the trends\n",
    "        trend_premonth = np.round(spy_data[\"MA_trend\"].asof(date_premonth), 4)\n",
    "        trend_transactiondate = np.round(spy_data[\"MA_trend\"].asof(trans_date), 4)\n",
    "        trend_onemonth = np.round(spy_data[\"MA_trend\"].asof(date_onemonth), 4)\n",
    "        trend_twomonth = np.round(spy_data[\"MA_trend\"].asof(date_twomonth), 4)\n",
    "        trend_threemonth = np.round(spy_data[\"MA_trend\"].asof(date_threemonth), 4)\n",
    "        trend_fourmonth = np.round(spy_data[\"MA_trend\"].asof(date_fourmonth), 4)\n",
    "        trend_fivemonth = np.round(spy_data[\"MA_trend\"].asof(date_fivemonth), 4)\n",
    "        trend_sixmonth = np.round(spy_data[\"MA_trend\"].asof(date_sixmonth), 4)\n",
    "        # print(trend_premonth,trend_transactiondate,trend_sixmonth)\n",
    "\n",
    "        # Get todays date\n",
    "        today = pd.to_datetime(date.today())\n",
    "\n",
    "        # Let's update the original dataframe\n",
    "        df.at[index, \"spy_price_-1month\"] = price_premonth\n",
    "        df.at[index, \"spy_price_transactiondate\"] = price_transactiondate\n",
    "\n",
    "        if date_onemonth < today:\n",
    "            df.at[index, \"spy_price_1month\"] = price_onemonth\n",
    "        if date_twomonth < today:\n",
    "            df.at[index, \"spy_price_2month\"] = price_twomonth\n",
    "        if date_threemonth < today:\n",
    "            df.at[index, \"spy_price_3month\"] = price_threemonth\n",
    "        if date_fourmonth < today:\n",
    "            df.at[index, \"spy_price_4month\"] = price_fourmonth\n",
    "        if date_fivemonth < today:\n",
    "            df.at[index, \"spy_price_5month\"] = price_fivemonth\n",
    "        if date_sixmonth < today:\n",
    "            df.at[index, \"spy_price_6month\"] = price_sixmonth\n",
    "\n",
    "        # Lets update the trend data\n",
    "        df.at[index, \"spy_trend_-1month\"] = trend_premonth\n",
    "        df.at[index, \"spy_trend_transactiondate\"] = trend_transactiondate\n",
    "        if date_onemonth < today:\n",
    "            df.at[index, \"spy_trend_1month\"] = trend_onemonth\n",
    "        if date_twomonth < today:\n",
    "            df.at[index, \"spy_trend_2month\"] = trend_twomonth\n",
    "        if date_threemonth < today:\n",
    "            df.at[index, \"spy_trend_3month\"] = trend_threemonth\n",
    "        if date_fourmonth < today:\n",
    "            df.at[index, \"spy_trend_4month\"] = trend_fourmonth\n",
    "        if date_fivemonth < today:\n",
    "            df.at[index, \"spy_trend_5month\"] = trend_fivemonth\n",
    "        if date_sixmonth < today:\n",
    "            df.at[index, \"spy_trend_6month\"] = trend_sixmonth\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the nature of this merge and only one yfinance call, this cell should run much faster with an approximate time of 5-6 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UV0xTFqmVCa"
   },
   "outputs": [],
   "source": [
    "temp_full1 = merge_spy_data(temp_full1, spy_data, ticker=\"SPY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 624,
     "status": "ok",
     "timestamp": 1749143387148,
     "user": {
      "displayName": "Thomas Macpherson",
      "userId": "08726222529195553341"
     },
     "user_tz": 360
    },
    "id": "5CoONbEImxU2",
    "outputId": "305750d0-6b8b-49f9-d2f1-86a090421d26"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the number of missing values in the file\n",
    "missing_counts = temp_full1.isna().sum()\n",
    "print(missing_counts)\n",
    "\n",
    "# Let's take a look at the min and max dates in here\n",
    "print(\n",
    "    f\"Min date: {np.min(temp_full1['Transaction Date'])}; Max date {np.max(temp_full1['Transaction Date'])}\"\n",
    ")\n",
    "\n",
    "temp_full1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tz0SZfaAcuXT"
   },
   "source": [
    "This was a successful way to use a dataset from one source (SEC) in order to merge data from a second data source (Yahoo Finance API). There are areas that could likely be optimized if more time permitted. For example, using a merge function for the SPY data based on dates, however due to the time constraint on this project we decided to save that as an extracurricular activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PY-ETUCgk9o"
   },
   "source": [
    "# Section 6: Save an Intermediate .CSV file\n",
    "Let's save an intermediate .CSV file that we can use for reproducibility in our study. This will allow anyone to run the EDA without having to spend hours obtaining the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1K1iK1cg2vv"
   },
   "outputs": [],
   "source": [
    "temp_full1.to_csv(\"notebook3_added_price_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "InsiderTrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
