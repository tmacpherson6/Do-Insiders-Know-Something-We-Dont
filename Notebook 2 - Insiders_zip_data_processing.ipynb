{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insider Transaction Processing\n",
    "This notebook is the second in our pipeline of analyzing insider transactions. In this notebook, our focus will be on extracting important infromation from all of the archived Form 4 ZIP files that we retrieved from the SEC website (See `Notebook 1 - donload_sec_zips.ipynb`). \n",
    "\n",
    "### Outline:  \n",
    "1) Load all SEC Form 4 filing ZIP archives (source:https://www.sec.gov/data-research/sec-markets-data/insider-transactions-data-sets) that have been downloaded locally to the dir: sec_zips.\n",
    "2) Extract and process three .tsv files ('NONDERIV_TRANS.tsv, 'REPORTINGOWNER.tsv','SUBMISSION.tsv'). We will filter down to only insider transactions (non-entity LPs, Funds, Trusts, etc.) and focus on open-market purchases. For details on the data contained in these ZIP files see 'insider_transactions_readme.pdf' in the github repository.  \n",
    "3) Clean data by removing any invalid records (missing roles, etc.)  \n",
    "4) Concatenate records from all ZIP files and export to a stable CSV file for future work\n",
    "\n",
    "Note: It is relatively easy to adjust this notebook for future analysis if we decide that we want to look at entities or do a deeper dive into why invalid records exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Libraries\n",
    "Let's load all of the libraries that we expect to use in this notebook. All libraries and versions are contained in the conda environment file environment.yml. If setting up a virtual environment for the first time, see `Virtual_Environment_Setup.ipynb` in the github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Obtaining Data and Concatenating\n",
    "In this section, we will iterate through all ZIP files and pull out the information that we need, we will put that information in a termorary dataframe. After iterating through all ZIP files we will then concatenate all of the dataframes together. Due to the fact that this is a loop, it will be a pretty large cell block of all the processing. I will explain the reasoning behind everything with inline commenting. I will split out and unnecessary code in the loop first to help with readability.\n",
    "\n",
    "Let's tart with finding all of the ZIP files that we have stored locally and creating a list oc them that we can use to iterate through. This notebook should be saved in the same directory as `Notebook 1` meaning that our current working directory should be accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 77 ZIP files in 'sec_zips', starting from: 2006q1_form345.zip and ending with: 2025q1_form345.zip\n"
     ]
    }
   ],
   "source": [
    "# Let's find our local directory where we stored the ZIP files\n",
    "local_zip_dir = \"sec_zips\"\n",
    "# In order to concatenate all files in order, we can sort this list\n",
    "all_files = sorted([f for f in os.listdir(local_zip_dir) if f.endswith(\".zip\")])\n",
    "print(\n",
    "    f\"Found {len(all_files)} ZIP files in '{local_zip_dir}', starting from: {all_files[0]} and ending with: {all_files[-1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we have identified the right directory and we can see how many records we will be iterating through to process. We can also see our earliest and most recent record for processing to make sure that they line up with our current date. Now, let's preprocess our final dataframe a little. Based on the 'insider_transaction_readme.pdf' file, we have identified columns that we think will be important to this study. These columns, like in many datasets, have titles that may not be very straight forward. So let's create two lists. One that is the selected columns that we want to pull from the documents, the other is a mapping dictionary that we can use to rename these columns in final processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a list to store the columns that we want for use in final_df and filtered_entities\n",
    "selected_columns = [\n",
    "    \"RPTOWNERNAME\",\n",
    "    \"RPTOWNER_TITLE\",\n",
    "    \"Insider Role\",\n",
    "    \"ISSUERNAME\",\n",
    "    \"ISSUERTRADINGSYMBOL\",\n",
    "    \"ISSUERCIK\",\n",
    "    \"PERIOD_OF_REPORT\",\n",
    "    \"TRANS_DATE\",\n",
    "    \"SECURITY_TITLE\",\n",
    "    \"TRANS_CODE\",\n",
    "    \"TRANS_SHARES\",\n",
    "    \"TRANS_PRICEPERSHARE\",\n",
    "    \"SHRS_OWND_FOLWNG_TRANS\",\n",
    "    \"DIRECT_INDIRECT_OWNERSHIP\",\n",
    "    \"ACCESSION_NUMBER\",\n",
    "]\n",
    "\n",
    "renaming_dict = {\n",
    "    \"RPTOWNERNAME\": \"Insider Name\",\n",
    "    \"RPTOWNER_TITLE\": \"Insider Title\",\n",
    "    \"Insider Role\": \"Insider Role\",\n",
    "    \"ISSUERNAME\": \"Issuer\",\n",
    "    \"ISSUERTRADINGSYMBOL\": \"Ticker\",\n",
    "    \"ISSUERCIK\": \"CIK Code\",\n",
    "    \"PERIOD_OF_REPORT\": \"Period of Report\",\n",
    "    \"TRANS_DATE\": \"Transaction Date\",\n",
    "    \"SECURITY_TITLE\": \"Security\",\n",
    "    \"TRANS_CODE\": \"Transaction Code\",\n",
    "    \"TRANS_SHARES\": \"Shares\",\n",
    "    \"TRANS_PRICEPERSHARE\": \"Price per Share\",\n",
    "    \"SHRS_OWND_FOLWNG_TRANS\": \"Shares After\",\n",
    "    \"DIRECT_INDIRECT_OWNERSHIP\": \"Ownership Type\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now that we have everything setup, we can start iterating through the ZIP files to process them. As stated before, I am going to use a lot of inline commenting for explanation through out this longer code block.  \n",
    "\n",
    "While exploring these files, I will talk a little about what each file contains. \n",
    "REPORTINGOWNER.tsv - Rows: Variable; Columns: 13  \n",
    "This report contains information related to the Insider. Has two unique keys as the accession number and a central index key of the reporting insider. It also contains the insiders name, role (officer, director, tenpercentowner, other), insider titel (CEO, CFO, VP, etc.), Additional details of position, street address, city, state, zip, description of state and then an SEC file number. We are looking for Open Market purchases which are indicated by the Transaction Code 'P'.\n",
    "\n",
    "NONDERIV_TRANS.tsv - Rows: Variable; Columns: 28  \n",
    "This file contains all the non-derivative (options, futures, etc.) transactions uses the accession number as key as well as a surrogate key. This contains information about the transaction like security title, transaction date, exccution date, transaction type, shares, Nature of ownership etc.\n",
    "\n",
    "SUBMISSION.tsv - Rows: Variable; Columns: 13  \n",
    "This form identifies the XML originating submissions, filer and issuer information again using the Aceession number as the primary key. This contains infomation about the filing_date, period_of_report, Symbol, etc.\n",
    "\n",
    "Note: This brings up a very valid concern in the validity of the study. We have been using transaction date and filling dates. However, there is a deemed execution date that may be more appropriate. It would be interesting to do an analysis and see if that results in a significant amount of retained price data that get's filtered out in Notebook 5.\n",
    "\n",
    "We will start by creating a list to store our dataframes for final concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 2006q1_form345.zip\n"
     ]
    }
   ],
   "source": [
    "# Create a list to store all merged DataFrames for each iteration\n",
    "merged_all = []\n",
    "\n",
    "# Loop through each ZIP file\n",
    "for zip_filename in all_files:\n",
    "    print(f\"Processing file: {zip_filename}\")\n",
    "    # Contruct the full path to the ZIP file\n",
    "    zip_path = os.path.join(local_zip_dir, zip_filename)\n",
    "    # Create a folder name by dropping the \".zip\" extension\n",
    "    folder_name = zip_filename.replace(\".zip\", \"\")\n",
    "    # Create a folder to extract the contents of the ZIP file\n",
    "    extract_path = f\"{local_zip_dir}/{folder_name}\"\n",
    "\n",
    "    # Extract the ZIP files but use an elegant Try/Except block to handle errors\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {zip_filename} due to extraction error: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Now that we have extracted the ZIP file, let's find the TSV files we want in the extracted folder\n",
    "    try:\n",
    "        nonderiv = pd.read_csv(\n",
    "            os.path.join(extract_path, \"NONDERIV_TRANS.tsv\"), sep=\"\\t\", low_memory=False\n",
    "        )  # used to suppress dtype warning\n",
    "        report = pd.read_csv(os.path.join(extract_path, \"REPORTINGOWNER.tsv\"), sep=\"\\t\")\n",
    "        submission = pd.read_csv(os.path.join(extract_path, \"SUBMISSION.tsv\"), sep=\"\\t\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {zip_filename} due to load error: {e}\")\n",
    "        continue\n",
    "\n",
    "    # In the original notebook we had a function get_role() which did not work, so this replaces it with a simpler approach\n",
    "    report[\"Insider Role\"] = report[\"RPTOWNER_RELATIONSHIP\"].str.strip().str.title()\n",
    "\n",
    "    # Now let's filter the non-deriv file for open-market buys \"P\" we can include Sales in the future \"S\"\"Insider Trading_ Do Corporate Insiders Know Something We Don't_.docx\"\n",
    "    filtered = nonderiv[\n",
    "        (nonderiv[\"SECURITY_TITLE\"].str.lower() == \"common stock\")\n",
    "        & (nonderiv[\"TRANS_CODE\"] == \"P\")\n",
    "    ]\n",
    "\n",
    "    # Let's also filter out any \"penny stocks\" in this case we will say any with a share price < $5\n",
    "    filtered = filtered[filtered[\"TRANS_PRICEPERSHARE\"] >= 5.0].copy()\n",
    "\n",
    "    # Here we are going to use a merge statement to join the filtered and the report data that we want\n",
    "    filtered = filtered.merge(\n",
    "        report[\n",
    "            [\n",
    "                \"ACCESSION_NUMBER\",\n",
    "                \"RPTOWNERNAME\",\n",
    "                \"RPTOWNER_TITLE\",\n",
    "                \"RPTOWNER_RELATIONSHIP\",\n",
    "                \"Insider Role\",\n",
    "            ]\n",
    "        ],\n",
    "        on=\"ACCESSION_NUMBER\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Now, let's create a copy to work on incase we mess anything up it will be easy to redo\n",
    "    before_entity_filter = filtered.copy()\n",
    "\n",
    "    # Let's convert the 'RPTOWNERNAME' to all uppercase for ease\n",
    "    filtered[\"RPTOWNERNAME\"] = filtered[\"RPTOWNERNAME\"].str.upper()\n",
    "\n",
    "    # Let's also create a list of entity_keywords that we want to search for\n",
    "    entity_keywords = [\n",
    "        \"LLC\",\n",
    "        \"L L C\",\n",
    "        \"L.L.C.\",\n",
    "        \"LP\",\n",
    "        \"L P\",\n",
    "        \"L.P.\",\n",
    "        \"LTD\",\n",
    "        \"INC\",\n",
    "        \"TRUST\",\n",
    "        \"CORP\",\n",
    "        \"FOUNDATION\",\n",
    "        \"COMPANY\",\n",
    "        \"CO\",\n",
    "        \"CO.\",\n",
    "        \"PARTNERS\",\n",
    "        \"ADVISORS\",\n",
    "        \"ADVISORY\",\n",
    "        \"CAPITAL\",\n",
    "        \"INVESTMENT\",\n",
    "        \"INVESTMENTS\",\n",
    "        \"HOLDINGS\",\n",
    "        \"MGMT\",\n",
    "        \"MANAGEMENT\",\n",
    "        \"FUND\",\n",
    "        \"GROUP\",\n",
    "        \"VENTURES\",\n",
    "        \"BIOVENTURES\",\n",
    "        \"INVESTORS\",\n",
    "        \"EQUITY\",\n",
    "        \"LIFE INSURANCE\",\n",
    "        \"GP\",\n",
    "        \"FAMILY\",\n",
    "        \"PBC\",\n",
    "        \"SDN BHD\",\n",
    "        \"GMBH\",\n",
    "    ]\n",
    "\n",
    "    # Now, let's create a regex pattern that detects keywordse with leading punctuation or spacing (to avoid names)\n",
    "    # For a full description of what this pattern does see `explanation of regex in Notebook2.docx`\n",
    "    pattern = \"(?i)\" + \"|\".join(\n",
    "        r\"(?<!\\w)\" + re.escape(k) + r\"(?=\\W|$)\" for k in entity_keywords\n",
    "    )\n",
    "\n",
    "    # Save the rows that will be the filtered out entities (for later review)\n",
    "    # filtered_out_df = before_entity_filter[before_entity_filter[\"RPTOWNERNAME\"].str.contains(pattern, case=False, na=False, regex=True)].copy()\n",
    "\n",
    "    # Merge the entity-filtered-out rows with submission info to align with final_df format\n",
    "    # filtered_out_df = filtered_out_df.merge(\n",
    "    #    submission[[\"ACCESSION_NUMBER\", \"ISSUERNAME\", \"ISSUERTRADINGSYMBOL\", \"PERIOD_OF_REPORT\", \"ISSUERCIK\"]],\n",
    "    #    on=\"ACCESSION_NUMBER\", how=\"left\"\n",
    "    # )\n",
    "\n",
    "    # Remove rows where the insider name matches any known entity keyword (e.g., LLC, INC, TRUST)\n",
    "    # Uses word boundaries to avoid false positives\n",
    "    filtered = filtered[\n",
    "        ~filtered[\"RPTOWNERNAME\"].str.contains(\n",
    "            pattern, case=False, na=False, regex=True\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Keep only valid insiders: director, officer, or has a job title, I may consider removing this line in the future\n",
    "    # .loc[;, ] used to address warning (means assign this transformation to every row in the column)\n",
    "    filtered.loc[:, \"RPTOWNER_RELATIONSHIP\"] = filtered[\n",
    "        \"RPTOWNER_RELATIONSHIP\"\n",
    "    ].str.upper()\n",
    "    filtered = filtered[\n",
    "        filtered[\"RPTOWNER_RELATIONSHIP\"].str.contains(\n",
    "            \"DIRECTOR|OFFICER|TENPERCENTOWNER\", na=False\n",
    "        )\n",
    "        | filtered[\"RPTOWNER_TITLE\"].notna()\n",
    "    ]\n",
    "\n",
    "    # Merge with submission to get equity issuer info\n",
    "    filtered = filtered.merge(\n",
    "        submission[\n",
    "            [\n",
    "                \"ACCESSION_NUMBER\",\n",
    "                \"ISSUERNAME\",\n",
    "                \"ISSUERTRADINGSYMBOL\",\n",
    "                \"PERIOD_OF_REPORT\",\n",
    "                \"ISSUERCIK\",  # Added \"ISSUECIK\" to map this field with SIC code\n",
    "            ]\n",
    "        ],\n",
    "        on=\"ACCESSION_NUMBER\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Filter out equity issuers that are investment funds\n",
    "    filtered = filtered[\n",
    "        ~filtered[\"ISSUERNAME\"].str.contains(\"FUND\", case=False, na=False)\n",
    "        & ~filtered[\"ISSUERNAME\"].str.contains(\"trust\", case=False, na=False)\n",
    "    ]\n",
    "\n",
    "    # Now we can rename output columns using our dictionary from earlier\n",
    "    final = filtered[selected_columns].rename(columns=renaming_dict)\n",
    "\n",
    "    # Append cleaned dataframe to master list\n",
    "    merged_all.append(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so we have been able to iterate through all of our ZIP files, combine the information necessary in three different .tsv files. Filter out some unnecessary insider transactions that are unimportant for the hypothesis we have developed for this project. We now have a list of 77 dataframes that we need to concatenate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 434020,
     "status": "ok",
     "timestamp": 1749136848151,
     "user": {
      "displayName": "Kirtland Corregan",
      "userId": "12271392998891066272"
     },
     "user_tz": 240
    },
    "id": "Bn1Q8ZGpqFlU",
    "outputId": "b9f5e714-2c29-4feb-8010-5a2f524bd3c6"
   },
   "outputs": [],
   "source": [
    "# Combine all cleaned rows into one DataFrame\n",
    "if merged_all:\n",
    "    final_df = pd.concat(merged_all, ignore_index=True)\n",
    "\n",
    "    # Save merged data\n",
    "    final_df.to_csv(\"notebook1_insider_data.csv\", index=False)\n",
    "    print(\"Saved merged data to notebook1_insider_data.csv\")\n",
    "\n",
    "    # Preview output\n",
    "    print(\"Preview of merged data:\")\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    display(final_df.head(10))\n",
    "    \n",
    "else:\n",
    "    print(\"No valid purchase data found in uploaded zip files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to have a list of all of the filtered out entities, we can uncomment this next block. For now, it will not be used in any analysis so we will keep it commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMJBUW53s7gF"
   },
   "outputs": [],
   "source": [
    "# Create CSV of filtered-out entities using same column names for records and future use\n",
    "#filtered_entities = filtered_out_df[selected_columns].rename(columns=renaming_dict)\n",
    "\n",
    "# Save to local drive\n",
    "#filtered_entities.to_csv(\"notebook1_filtered_out_entities.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can proceed to `Notebook 3 = yahoo_finance_price_data.ipynb` where we will use this large .CSV file in order to query the Yahoo! Finance API, 'yinance' to get historical price data for the 7 months in and around the insider transaction date."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "InsiderTrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
